{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure LLM Wrapper Usage Example\n",
    "\n",
    "This notebook demonstrates how to use the Azure LLM wrapper module for cleaner, more maintainable code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, ensure you have created a `.env` file in the project root with your Azure credentials.\n",
    "\n",
    "```env\n",
    "AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/\n",
    "AZURE_OPENAI_API_KEY=your-key-here\n",
    "AZURE_OPENAI_API_VERSION=2024-02-01\n",
    "AZURE_OPENAI_CHAT_DEPLOYMENT_NAME=gpt-4.1-mini\n",
    "AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT_NAME=text-embedding-3-large\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from llm_wrapper import get_chat_llm, get_embeddings, AzureLLMWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Basic Chat Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not configure Opik tracing: API key must be set to check workspace name.\n",
      "âœ“ Opik tracer initialized for project: customer-support\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rushi\\AppData\\Local\\Temp\\ipykernel_36164\\3033278527.py:8: UserWarning: WARNING! enable_tracing is not default parameter.\n",
      "                enable_tracing was transferred to model_kwargs.\n",
      "                Please confirm that enable_tracing is what you intended.\n",
      "  llm = get_chat_llm(temperature=0.7, max_tokens=500, enable_tracing=True)\n"
     ]
    }
   ],
   "source": [
    "# from opik import configure \n",
    "# from opik.integrations.langchain import OpikTracer \n",
    "\n",
    "# configure() \n",
    "\n",
    "# opik_tracer = OpikTracer() \n",
    "\n",
    "llm = get_chat_llm(temperature=0.7, max_tokens=500, enable_tracing=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "# Get a chat LLM with the convenience function\n",
    "llm = get_chat_llm(temperature=0.7, max_tokens=500)\n",
    "\n",
    "# Use it\n",
    "response = llm.invoke(\"What is the capital of France?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AzureChatOpenAI(profile={'max_input_tokens': 1047576, 'max_output_tokens': 32768, 'image_inputs': True, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': True, 'structured_output': True, 'image_url_inputs': True, 'pdf_inputs': True, 'pdf_tool_message': True, 'image_tool_message': True, 'tool_choice': True}, client=<openai.resources.chat.completions.completions.Completions object at 0x0000023489A53EC0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000002348AE6FB00>, root_client=<openai.lib.azure.AzureOpenAI object at 0x00000234894D1C40>, root_async_client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000002348ABDE450>, model_name='gpt-4.1-mini', temperature=0.7, model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True, max_tokens=500, disabled_params={'parallel_tool_calls': None}, azure_endpoint='https://rushi-m9xyt0et-eastus2.cognitiveservices.azure.com/', deployment_name='gpt-4.1-mini', openai_api_version='2024-02-01', openai_api_type='azure')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Using with LangChain Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "llm = get_chat_llm(temperature=0.7, max_tokens=1000)\n",
    "\n",
    "system_message = SystemMessage(\n",
    "    content=\"You are a helpful assistant that provides accurate information based on the provided context.\"\n",
    ")\n",
    "human_message = HumanMessage(content=\"Explain what AI embeddings are.\")\n",
    "\n",
    "response = llm.invoke([system_message, human_message])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings instance\n",
    "embeddings = get_embeddings()\n",
    "\n",
    "# Embed a query\n",
    "query_embedding = embeddings.embed_query(\"What is the capital of France?\")\n",
    "print(f\"Embedding dimension: {len(query_embedding)}\")\n",
    "print(f\"First 5 values: {query_embedding[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Refactored PDF Parsing with RAG\n",
    "\n",
    "This is the refactored version of the original pdf_parsing.ipynb notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "import pymupdf4llm\n",
    "\n",
    "# Use the wrapper for embeddings\n",
    "embeddings = get_embeddings()\n",
    "\n",
    "pdf_path = '../SmartScriblle.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse PDF with markdown structure\n",
    "md_text = pymupdf4llm.to_markdown(pdf_path)\n",
    "\n",
    "# Split by markdown headers\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "md_header_splits = markdown_splitter.split_text(md_text)\n",
    "\n",
    "print(f\"Split into {len(md_header_splits)} sections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup vector store with our embeddings\n",
    "from langchain_postgres import PGVector\n",
    "\n",
    "connection = \"postgresql+psycopg://langchain:langchain@localhost:6024/langchain\"\n",
    "collection_name = \"smart_scribble_docs\"\n",
    "\n",
    "vector_store = PGVector(\n",
    "    embeddings=embeddings,  # Using embeddings from wrapper\n",
    "    collection_name=collection_name,\n",
    "    connection=connection,\n",
    "    use_jsonb=True,\n",
    ")\n",
    "\n",
    "# Add documents\n",
    "vector_store.add_documents(md_header_splits, ids=[id for id in range(len(md_header_splits))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the vector store\n",
    "user_query = 'give me the technical specifications'\n",
    "\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "chunks = retriever.invoke(user_query)\n",
    "\n",
    "# Build prompt with context\n",
    "context = ' '.join([chunk.page_content for chunk in chunks])\n",
    "prompt = f\"User Query: {user_query}\\n\\nContext: {context}\"\n",
    "\n",
    "print(f\"Retrieved {len(chunks)} relevant chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the LLM wrapper instead of manual configuration\n",
    "llm = get_chat_llm(temperature=0.7, max_tokens=1000)\n",
    "\n",
    "system_message = SystemMessage(\n",
    "    content=\"You are a helpful assistant that provides accurate information based on the provided context, limit yourself to only the requested user queries response\"\n",
    ")\n",
    "human_message = HumanMessage(content=prompt)\n",
    "\n",
    "ai_response = llm.invoke([system_message, human_message])\n",
    "print(ai_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5: Using the Wrapper Class for Advanced Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create wrapper instance\n",
    "wrapper = AzureLLMWrapper()\n",
    "\n",
    "# Get different LLM configurations for different purposes\n",
    "creative_llm = wrapper.get_chat_llm(temperature=1.0, max_tokens=300)\n",
    "factual_llm = wrapper.get_chat_llm(temperature=0.2, max_tokens=200)\n",
    "\n",
    "prompt = \"Describe the SmartScribble AI Notebook\"\n",
    "\n",
    "print(\"Creative description:\")\n",
    "print(creative_llm.invoke(prompt).content)\n",
    "\n",
    "print(\"\\nFactual description:\")\n",
    "print(factual_llm.invoke(prompt).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 6: Creating a Retrieval Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def retriever_tool(user_query: str) -> str:\n",
    "    \"\"\"\n",
    "    Retrieve relevant document chunks based on the user query.\n",
    "    Context is related to information about the SmartScribble product \n",
    "    like details, technical specifications, features, compatibility, pricing, and availability.\n",
    "    \"\"\"\n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "    chunks = retriever.invoke(user_query)\n",
    "    return ' '.join([chunk.page_content for chunk in chunks])\n",
    "\n",
    "# Test the tool\n",
    "context = retriever_tool.invoke(\"What is the battery life?\")\n",
    "print(f\"Retrieved context: {context[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benefits of Using the Wrapper\n",
    "\n",
    "1. **Cleaner code**: No need to manually set environment variables\n",
    "2. **Reusability**: Import and use across multiple notebooks and modules\n",
    "3. **Configuration management**: All config in one place (.env file)\n",
    "4. **Type safety**: Better IDE support with type hints\n",
    "5. **Error handling**: Built-in validation of required configurations\n",
    "6. **Consistency**: Same configuration across all your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison: Before vs After\n",
    "\n",
    "### Before (Manual Configuration):\n",
    "```python\n",
    "import os\n",
    "os.environ['AZURE_OPENAI_ENDPOINT'] = '<endpoint>'\n",
    "os.environ['AZURE_OPENAI_API_EMBEDDINGS_DEPLOYMENT_NAME']='text-embedding-3-large'\n",
    "os.environ['AZURE_OPENAI_API_KEY']='<key>'\n",
    "os.environ['OPENAI_API_VERSION']=\"2024-02-01\"\n",
    "\n",
    "from langchain_openai.embeddings.azure import AzureOpenAIEmbeddings\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=\"text-embedding-3-large\",\n",
    "    model=\"text-embedding-3-large\"\n",
    ")\n",
    "```\n",
    "\n",
    "### After (Using Wrapper):\n",
    "```python\n",
    "from llm_wrapper import get_embeddings\n",
    "embeddings = get_embeddings()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
